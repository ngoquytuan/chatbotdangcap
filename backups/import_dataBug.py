# import_data2.py
import os
import json
import logging
import sqlite3
from pathlib import Path
from typing import Dict, Any

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from colorama import Fore, Style, init as colorama_init

# ==== CONFIG ====
DB_PATH = "rag_system/data/metadata.db"
INDEX_PATH = "rag_system/data/indexes/index.faiss"
JSON_DIR = "rag_system/data/ingested_json"
MODEL_NAME = "AITeamVN/Vietnamese_Embedding" # Cáº§n cho viá»‡c láº¥y sá»‘ chiá»u embedding
USE_GPU = True  # True náº¿u muá»‘n GPU

# ==== INIT COLOR LOG ====
colorama_init(autoreset=True)
logging.basicConfig(level=logging.INFO, format="%(message)s")

def log_info(msg):
    logging.info(Fore.CYAN + msg + Style.RESET_ALL)

def log_success(msg):
    logging.info(Fore.GREEN + msg + Style.RESET_ALL)

def log_warn(msg):
    logging.warning(Fore.YELLOW + msg + Style.RESET_ALL)

def log_error(msg):
    logging.error(Fore.RED + msg + Style.RESET_ALL)

# ==== DB MANAGER ====
class DBManager:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        self.ensure_schema()

    def ensure_schema(self):
        # Sá»­ dá»¥ng TEXT cho embedding Ä‘á»ƒ lÆ°u chuá»—i JSON
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                chunk_id TEXT PRIMARY KEY,
                document_id TEXT,
                text TEXT,
                embedding TEXT
            )
        """)
        self.conn.commit()

    def insert_chunk(self, chunk: Dict[str, Any]) -> bool:
        try:
            # FIX: Chuyá»ƒn Ä‘á»•i numpy array thÃ nh list, sau Ä‘Ã³ thÃ nh chuá»—i JSON
            embedding_json = json.dumps(chunk["embedding"].tolist())
            self.conn.execute("""
                INSERT OR IGNORE INTO chunks (chunk_id, document_id, text, embedding)
                VALUES (?, ?, ?, ?)
            """, (
                chunk["chunk_id"],
                chunk["document_id"],
                chunk["text"],
                embedding_json # LÆ°u chuá»—i JSON
            ))
            self.conn.commit()
            return True
        except Exception as e:
            log_error(f"Lá»—i insert chunk {chunk['chunk_id']}: {e}")
            return False

    def chunk_exists(self, chunk_id: str) -> bool:
        cur = self.conn.execute("SELECT 1 FROM chunks WHERE chunk_id = ?", (chunk_id,))
        return cur.fetchone() is not None

    def close(self):
        self.conn.close()

# ==== MAIN IMPORT FUNCTION ====
def main():
    log_info("ğŸš€ Khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding Ä‘á»ƒ láº¥y sá»‘ chiá»u...")
    device = "cuda" if USE_GPU else "cpu"
    model = SentenceTransformer(MODEL_NAME, device=device)
    dim = model.get_sentence_embedding_dimension()
    log_success(f"âœ… Sá»‘ chiá»u embedding: {dim}")

    # Khá»Ÿi táº¡o FAISS index
    log_info("ğŸ“¦ Khá»Ÿi táº¡o FAISS index...")
    # DÃ¹ng IndexFlatIP (Inner Product) vÃ¬ embedding Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a (normalize_embeddings=True trong ingestion)
    index = faiss.IndexFlatIP(dim) 
    index = faiss.IndexIDMap2(index)

    # Káº¿t ná»‘i DB
    db = DBManager(DB_PATH)

    files = list(Path(JSON_DIR).glob("*.json"))
    if not files:
        log_warn("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON nÃ o Ä‘á»ƒ import.")
        return

    stats = {"files_ok": 0, "files_err": 0, "chunks_inserted": 0, "chunks_skipped": 0}

    for file in files:
        log_info(f"ğŸ“‚ Xá»­ lÃ½ file: {file.name}")
        try:
            with open(file, "r", encoding="utf-8") as f:
                data = json.load(f)

            if "chunks" not in data or not isinstance(data["chunks"], list):
                log_warn(f"âš ï¸ File {file.name} khÃ´ng cÃ³ trÆ°á»ng 'chunks'")
                stats["files_err"] += 1
                continue

            for chunk in data["chunks"]:
                chunk_id = chunk.get("chunk_id")
                if not chunk_id or "embedding" not in chunk:
                    log_warn(f"âš ï¸ Chunk trong {file.name} thiáº¿u dá»¯ liá»‡u báº¯t buá»™c (chunk_id, embedding).")
                    stats["chunks_skipped"] += 1
                    continue

                if db.chunk_exists(chunk_id):
                    log_warn(f"âš ï¸ Chunk {chunk_id} Ä‘Ã£ tá»“n táº¡i, bá» qua.")
                    stats["chunks_skipped"] += 1
                    continue
                
                vec = np.array(chunk["embedding"], dtype="float32")
                if vec.shape[0] != dim:
                    log_warn(f"âš ï¸ Chunk {chunk_id} cÃ³ dimension {vec.shape[0]} â‰  {dim}, bá» qua.")
                    stats["chunks_skipped"] += 1
                    continue

                # Táº¡o ID cho FAISS tá»« hash cá»§a chunk_id
                # DÃ¹ng modulo Ä‘á»ƒ Ä‘áº£m báº£o ID náº±m trong pháº¡m vi cá»§a int64
                faiss_id = hash(chunk_id) % (2**63 - 1)
                
                # ThÃªm vÃ o DB vÃ  FAISS index
                if db.insert_chunk({
                    "chunk_id": chunk_id,
                    "document_id": chunk.get("document_id", ""),
                    "text": chunk.get("text", ""),
                    "embedding": vec
                }):
                    # FAISS yÃªu cáº§u vector pháº£i lÃ  2D array
                    index.add_with_ids(vec.reshape(1, -1), np.array([faiss_id], dtype="int64"))
                    stats["chunks_inserted"] += 1
                else:
                    # Náº¿u insert vÃ o DB lá»—i thÃ¬ cÅ©ng khÃ´ng thÃªm vÃ o FAISS
                    stats["chunks_skipped"] += 1


            stats["files_ok"] += 1

        except Exception as e:
            log_error(f"âŒ Lá»—i khi xá»­ lÃ½ {file.name}: {e}")
            stats["files_err"] += 1

    # LÆ°u FAISS index
    if stats["chunks_inserted"] > 0:
        os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
        faiss.write_index(index, INDEX_PATH)
        log_success(f"ğŸ’¾ ÄÃ£ lÆ°u FAISS index vÃ o {INDEX_PATH}")
    else:
        log_warn("âš ï¸ KhÃ´ng cÃ³ chunk nÃ o Ä‘Æ°á»£c thÃªm vÃ o, khÃ´ng táº¡o file FAISS index.")

    db.close()

    log_info("\nğŸ“Š **BÃO CÃO Tá»”NG Káº¾T**")
    log_success(f"  âœ” Files thÃ nh cÃ´ng: {stats['files_ok']}")
    log_error(f"  âœ– Files lá»—i: {stats['files_err']}")
    log_success(f"  âœ” Chunks thÃªm má»›i: {stats['chunks_inserted']}")
    log_warn(f"  âš ï¸ Chunks bá» qua: {stats['chunks_skipped']}")

if __name__ == "__main__":
    main()