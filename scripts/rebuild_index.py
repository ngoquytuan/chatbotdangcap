# rebuild_index.py
import os
import sqlite3
import json
import logging
from pathlib import Path

import faiss
import numpy as np
from colorama import Fore, Style, init as colorama_init

# ==== CONFIG ====
DB_PATH = "rag_system/data/metadata.db"
INDEX_PATH = "rag_system/data/indexes/index.faiss"
# L·∫•y t·ª´ file readme.json ho·∫∑c t·ª´ model.get_sentence_embedding_dimension()
EMBEDDING_DIM = 1024 

# ==== INIT COLOR LOG ====
colorama_init(autoreset=True)
logging.basicConfig(level=logging.INFO, format="%(message)s")

def log_info(msg):
    logging.info(Fore.CYAN + msg + Style.RESET_ALL)

def log_success(msg):
    logging.info(Fore.GREEN + msg + Style.RESET_ALL)

def log_warn(msg):
    logging.warning(Fore.YELLOW + msg + Style.RESET_ALL)

def log_error(msg):
    logging.error(Fore.RED + msg + Style.RESET_ALL)

def rebuild_faiss_index():
    """
    ƒê·ªçc t·∫•t c·∫£ c√°c chunk t·ª´ SQLite v√† x√¢y d·ª±ng l·∫°i FAISS index.
    """
    log_info(f"üîç B·∫Øt ƒë·∫ßu qu√° tr√¨nh x√¢y d·ª±ng l·∫°i FAISS index t·ª´ '{DB_PATH}'...")

    db_file = Path(DB_PATH)
    if not db_file.exists():
        log_error(f"‚ùå Kh√¥ng t√¨m th·∫•y file database t·∫°i: {DB_PATH}")
        return

    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT chunk_id, embedding FROM chunks")
        rows = cursor.fetchall()
    except Exception as e:
        log_error(f"‚ùå L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ database: {e}")
        return
    finally:
        if 'conn' in locals() and conn:
            conn.close()

    if not rows:
        log_warn("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong database ƒë·ªÉ x√¢y d·ª±ng index.")
        return

    log_info(f"üìö T√¨m th·∫•y {len(rows)} chunks trong database.")

    # Kh·ªüi t·∫°o FAISS index
    log_info(f"üì¶ Kh·ªüi t·∫°o FAISS index v·ªõi dimension: {EMBEDDING_DIM}")
    index = faiss.IndexFlatIP(EMBEDDING_DIM)
    index = faiss.IndexIDMap2(index)

    vectors = []
    ids = []

    for row in rows:
        try:
            # FIX: ƒê·ªçc chu·ªói JSON t·ª´ DB, chuy·ªÉn th√†nh list, r·ªìi th√†nh numpy array
            embedding_list = json.loads(row['embedding'])
            vector = np.array(embedding_list, dtype="float32")
            
            # Ki·ªÉm tra s·ªë chi·ªÅu
            if vector.shape[0] != EMBEDDING_DIM:
                log_warn(f"‚ö†Ô∏è Chunk {row['chunk_id']} c√≥ dimension kh√¥ng h·ª£p l·ªá ({vector.shape[0]}), b·ªè qua.")
                continue

            # T·∫°o FAISS ID t·ª´ hash c·ªßa chunk_id, ƒë·ªìng b·ªô v·ªõi logic c·ªßa import_data2.py
            faiss_id = hash(row['chunk_id']) % (2**63 - 1)

            vectors.append(vector)
            ids.append(faiss_id)

        except (json.JSONDecodeError, TypeError) as e:
            log_warn(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω embedding cho chunk {row['chunk_id']}: {e}. B·ªè qua.")
            continue
    
    if not vectors:
        log_error("‚ùå Kh√¥ng c√≥ vector h·ª£p l·ªá n√†o ƒë·ªÉ th√™m v√†o index.")
        return

    log_info(f"‚ûï Th√™m {len(vectors)} vectors v√†o index...")
    
    # Chuy·ªÉn ƒë·ªïi list th√†nh numpy array 2D
    vectors_np = np.array(vectors, dtype="float32")
    ids_np = np.array(ids, dtype="int64")

    # Th√™m v√†o index
    index.add_with_ids(vectors_np, ids_np)

    # L∆∞u FAISS index
    os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
    faiss.write_index(index, INDEX_PATH)
    log_success(f"üíæ ƒê√£ l∆∞u FAISS index m·ªõi v√†o '{INDEX_PATH}' th√†nh c√¥ng!")
    log_info(f"‚ú® Index ch·ª©a t·ªïng c·ªông {index.ntotal} vectors.")

if __name__ == "__main__":
    rebuild_faiss_index()