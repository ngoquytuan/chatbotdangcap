Ch·∫Øc ch·∫Øn r·ªìi. D∆∞·ªõi ƒë√¢y l√† to√†n b·ªô n·ªôi dung cho t·ªáp `import_data2.py` ƒë√£ ƒë∆∞·ª£c s·ª≠a l·ªói v√† t·ªáp `rebuild_index.py` ƒë∆∞·ª£c t·∫°o m·ªõi ƒë·ªÉ b·∫°n s·ª≠ d·ª•ng.

-----

### **1. T·ªáp `import_data2.py` (ƒê√£ s·ª≠a l·ªói)**

ƒê√¢y l√† to√†n b·ªô n·ªôi dung c·ªßa t·ªáp `import_data2.py`. B·∫°n ch·ªâ c·∫ßn sao ch√©p v√† ghi ƒë√® l√™n t·ªáp c≈© c·ªßa m√¨nh. Thay ƒë·ªïi quan tr·ªçng n·∫±m ·ªü d√≤ng 56, n∆°i ch√∫ng t√¥i chuy·ªÉn ƒë·ªïi embedding th√†nh chu·ªói JSON tr∆∞·ªõc khi l∆∞u.

```python
# import_data2.py
import os
import json
import logging
import sqlite3
from pathlib import Path
from typing import Dict, Any

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from colorama import Fore, Style, init as colorama_init

# ==== CONFIG ====
DB_PATH = "rag_system/data/metadata.db"
INDEX_PATH = "rag_system/data/indexes/index.faiss"
JSON_DIR = "rag_system/data/ingested_json"
MODEL_NAME = "AITeamVN/Vietnamese_Embedding" # C·∫ßn cho vi·ªác l·∫•y s·ªë chi·ªÅu embedding
USE_GPU = True  # True n·∫øu mu·ªën GPU

# ==== INIT COLOR LOG ====
colorama_init(autoreset=True)
logging.basicConfig(level=logging.INFO, format="%(message)s")

def log_info(msg):
    logging.info(Fore.CYAN + msg + Style.RESET_ALL)

def log_success(msg):
    logging.info(Fore.GREEN + msg + Style.RESET_ALL)

def log_warn(msg):
    logging.warning(Fore.YELLOW + msg + Style.RESET_ALL)

def log_error(msg):
    logging.error(Fore.RED + msg + Style.RESET_ALL)

# ==== DB MANAGER ====
class DBManager:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        self.ensure_schema()

    def ensure_schema(self):
        # S·ª≠ d·ª•ng TEXT cho embedding ƒë·ªÉ l∆∞u chu·ªói JSON
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                chunk_id TEXT PRIMARY KEY,
                document_id TEXT,
                text TEXT,
                embedding TEXT
            )
        """)
        self.conn.commit()

    def insert_chunk(self, chunk: Dict[str, Any]) -> bool:
        try:
            # FIX: Chuy·ªÉn ƒë·ªïi numpy array th√†nh list, sau ƒë√≥ th√†nh chu·ªói JSON
            embedding_json = json.dumps(chunk["embedding"].tolist())
            self.conn.execute("""
                INSERT OR IGNORE INTO chunks (chunk_id, document_id, text, embedding)
                VALUES (?, ?, ?, ?)
            """, (
                chunk["chunk_id"],
                chunk["document_id"],
                chunk["text"],
                embedding_json # L∆∞u chu·ªói JSON
            ))
            self.conn.commit()
            return True
        except Exception as e:
            log_error(f"L·ªói insert chunk {chunk['chunk_id']}: {e}")
            return False

    def chunk_exists(self, chunk_id: str) -> bool:
        cur = self.conn.execute("SELECT 1 FROM chunks WHERE chunk_id = ?", (chunk_id,))
        return cur.fetchone() is not None

    def close(self):
        self.conn.close()

# ==== MAIN IMPORT FUNCTION ====
def main():
    log_info("üöÄ Kh·ªüi t·∫°o m√¥ h√¨nh embedding ƒë·ªÉ l·∫•y s·ªë chi·ªÅu...")
    device = "cuda" if USE_GPU else "cpu"
    model = SentenceTransformer(MODEL_NAME, device=device)
    dim = model.get_sentence_embedding_dimension()
    log_success(f"‚úÖ S·ªë chi·ªÅu embedding: {dim}")

    # Kh·ªüi t·∫°o FAISS index
    log_info("üì¶ Kh·ªüi t·∫°o FAISS index...")
    # D√πng IndexFlatIP (Inner Product) v√¨ embedding ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a (normalize_embeddings=True trong ingestion)
    index = faiss.IndexFlatIP(dim) 
    index = faiss.IndexIDMap2(index)

    # K·∫øt n·ªëi DB
    db = DBManager(DB_PATH)

    files = list(Path(JSON_DIR).glob("*.json"))
    if not files:
        log_warn("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON n√†o ƒë·ªÉ import.")
        return

    stats = {"files_ok": 0, "files_err": 0, "chunks_inserted": 0, "chunks_skipped": 0}

    for file in files:
        log_info(f"üìÇ X·ª≠ l√Ω file: {file.name}")
        try:
            with open(file, "r", encoding="utf-8") as f:
                data = json.load(f)

            if "chunks" not in data or not isinstance(data["chunks"], list):
                log_warn(f"‚ö†Ô∏è File {file.name} kh√¥ng c√≥ tr∆∞·ªùng 'chunks'")
                stats["files_err"] += 1
                continue

            for chunk in data["chunks"]:
                chunk_id = chunk.get("chunk_id")
                if not chunk_id or "embedding" not in chunk:
                    log_warn(f"‚ö†Ô∏è Chunk trong {file.name} thi·∫øu d·ªØ li·ªáu b·∫Øt bu·ªôc (chunk_id, embedding).")
                    stats["chunks_skipped"] += 1
                    continue

                if db.chunk_exists(chunk_id):
                    log_warn(f"‚ö†Ô∏è Chunk {chunk_id} ƒë√£ t·ªìn t·∫°i, b·ªè qua.")
                    stats["chunks_skipped"] += 1
                    continue
                
                vec = np.array(chunk["embedding"], dtype="float32")
                if vec.shape[0] != dim:
                    log_warn(f"‚ö†Ô∏è Chunk {chunk_id} c√≥ dimension {vec.shape[0]} ‚â† {dim}, b·ªè qua.")
                    stats["chunks_skipped"] += 1
                    continue

                # T·∫°o ID cho FAISS t·ª´ hash c·ªßa chunk_id
                # D√πng modulo ƒë·ªÉ ƒë·∫£m b·∫£o ID n·∫±m trong ph·∫°m vi c·ªßa int64
                faiss_id = hash(chunk_id) % (2**63 - 1)
                
                # Th√™m v√†o DB v√† FAISS index
                if db.insert_chunk({
                    "chunk_id": chunk_id,
                    "document_id": chunk.get("document_id", ""),
                    "text": chunk.get("text", ""),
                    "embedding": vec
                }):
                    # FAISS y√™u c·∫ßu vector ph·∫£i l√† 2D array
                    index.add_with_ids(vec.reshape(1, -1), np.array([faiss_id], dtype="int64"))
                    stats["chunks_inserted"] += 1
                else:
                    # N·∫øu insert v√†o DB l·ªói th√¨ c≈©ng kh√¥ng th√™m v√†o FAISS
                    stats["chunks_skipped"] += 1


            stats["files_ok"] += 1

        except Exception as e:
            log_error(f"‚ùå L·ªói khi x·ª≠ l√Ω {file.name}: {e}")
            stats["files_err"] += 1

    # L∆∞u FAISS index
    if stats["chunks_inserted"] > 0:
        os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
        faiss.write_index(index, INDEX_PATH)
        log_success(f"üíæ ƒê√£ l∆∞u FAISS index v√†o {INDEX_PATH}")
    else:
        log_warn("‚ö†Ô∏è Kh√¥ng c√≥ chunk n√†o ƒë∆∞·ª£c th√™m v√†o, kh√¥ng t·∫°o file FAISS index.")

    db.close()

    log_info("\nüìä **B√ÅO C√ÅO T·ªîNG K·∫æT**")
    log_success(f"  ‚úî Files th√†nh c√¥ng: {stats['files_ok']}")
    log_error(f"  ‚úñ Files l·ªói: {stats['files_err']}")
    log_success(f"  ‚úî Chunks th√™m m·ªõi: {stats['chunks_inserted']}")
    log_warn(f"  ‚ö†Ô∏è Chunks b·ªè qua: {stats['chunks_skipped']}")

if __name__ == "__main__":
    main()
```

-----

### **2. T·ªáp `rebuild_index.py` (N·ªôi dung m·ªõi)**

ƒê√¢y l√† m·ªôt script ho√†n to√†n m·ªõi gi√∫p b·∫°n x√¢y d·ª±ng l·∫°i t·ªáp `index.faiss` t·ª´ c∆° s·ªü d·ªØ li·ªáu `metadata.db` ƒë√£ ƒë∆∞·ª£c s·ª≠a l·ªói. ƒêi·ªÅu n√†y r·∫•t h·ªØu √≠ch khi t·ªáp index b·ªã h·ªèng ho·∫∑c c·∫ßn ƒë∆∞·ª£c t·∫°o l·∫°i m√† kh√¥ng c·∫ßn ph·∫£i ch·∫°y l·∫°i to√†n b·ªô qu√° tr√¨nh ingestion.

  * **T·∫°o m·ªôt t·ªáp m·ªõi** c√≥ t√™n `rebuild_index.py` trong th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n c·ªßa b·∫°n.
  * **D√°n n·ªôi dung sau v√†o t·ªáp:**

<!-- end list -->

```python
# rebuild_index.py
import os
import sqlite3
import json
import logging
from pathlib import Path

import faiss
import numpy as np
from colorama import Fore, Style, init as colorama_init

# ==== CONFIG ====
DB_PATH = "rag_system/data/metadata.db"
INDEX_PATH = "rag_system/data/indexes/index.faiss"
# L·∫•y t·ª´ file readme.json ho·∫∑c t·ª´ model.get_sentence_embedding_dimension()
EMBEDDING_DIM = 1024 

# ==== INIT COLOR LOG ====
colorama_init(autoreset=True)
logging.basicConfig(level=logging.INFO, format="%(message)s")

def log_info(msg):
    logging.info(Fore.CYAN + msg + Style.RESET_ALL)

def log_success(msg):
    logging.info(Fore.GREEN + msg + Style.RESET_ALL)

def log_warn(msg):
    logging.warning(Fore.YELLOW + msg + Style.RESET_ALL)

def log_error(msg):
    logging.error(Fore.RED + msg + Style.RESET_ALL)

def rebuild_faiss_index():
    """
    ƒê·ªçc t·∫•t c·∫£ c√°c chunk t·ª´ SQLite v√† x√¢y d·ª±ng l·∫°i FAISS index.
    """
    log_info(f"üîç B·∫Øt ƒë·∫ßu qu√° tr√¨nh x√¢y d·ª±ng l·∫°i FAISS index t·ª´ '{DB_PATH}'...")

    db_file = Path(DB_PATH)
    if not db_file.exists():
        log_error(f"‚ùå Kh√¥ng t√¨m th·∫•y file database t·∫°i: {DB_PATH}")
        return

    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT chunk_id, embedding FROM chunks")
        rows = cursor.fetchall()
    except Exception as e:
        log_error(f"‚ùå L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ database: {e}")
        return
    finally:
        if 'conn' in locals() and conn:
            conn.close()

    if not rows:
        log_warn("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong database ƒë·ªÉ x√¢y d·ª±ng index.")
        return

    log_info(f"üìö T√¨m th·∫•y {len(rows)} chunks trong database.")

    # Kh·ªüi t·∫°o FAISS index
    log_info(f"üì¶ Kh·ªüi t·∫°o FAISS index v·ªõi dimension: {EMBEDDING_DIM}")
    index = faiss.IndexFlatIP(EMBEDDING_DIM)
    index = faiss.IndexIDMap2(index)

    vectors = []
    ids = []

    for row in rows:
        try:
            # FIX: ƒê·ªçc chu·ªói JSON t·ª´ DB, chuy·ªÉn th√†nh list, r·ªìi th√†nh numpy array
            embedding_list = json.loads(row['embedding'])
            vector = np.array(embedding_list, dtype="float32")
            
            # Ki·ªÉm tra s·ªë chi·ªÅu
            if vector.shape[0] != EMBEDDING_DIM:
                log_warn(f"‚ö†Ô∏è Chunk {row['chunk_id']} c√≥ dimension kh√¥ng h·ª£p l·ªá ({vector.shape[0]}), b·ªè qua.")
                continue

            # T·∫°o FAISS ID t·ª´ hash c·ªßa chunk_id, ƒë·ªìng b·ªô v·ªõi logic c·ªßa import_data2.py
            faiss_id = hash(row['chunk_id']) % (2**63 - 1)

            vectors.append(vector)
            ids.append(faiss_id)

        except (json.JSONDecodeError, TypeError) as e:
            log_warn(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω embedding cho chunk {row['chunk_id']}: {e}. B·ªè qua.")
            continue
    
    if not vectors:
        log_error("‚ùå Kh√¥ng c√≥ vector h·ª£p l·ªá n√†o ƒë·ªÉ th√™m v√†o index.")
        return

    log_info(f"‚ûï Th√™m {len(vectors)} vectors v√†o index...")
    
    # Chuy·ªÉn ƒë·ªïi list th√†nh numpy array 2D
    vectors_np = np.array(vectors, dtype="float32")
    ids_np = np.array(ids, dtype="int64")

    # Th√™m v√†o index
    index.add_with_ids(vectors_np, ids_np)

    # L∆∞u FAISS index
    os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
    faiss.write_index(index, INDEX_PATH)
    log_success(f"üíæ ƒê√£ l∆∞u FAISS index m·ªõi v√†o '{INDEX_PATH}' th√†nh c√¥ng!")
    log_info(f"‚ú® Index ch·ª©a t·ªïng c·ªông {index.ntotal} vectors.")

if __name__ == "__main__":
    rebuild_faiss_index()

```

B√¢y gi·ªù, quy tr√¨nh l√†m vi·ªác c·ªßa b·∫°n s·∫Ω l√†:

1.  **M·ªôt l·∫ßn duy nh·∫•t:** X√≥a `metadata.db` v√† `index.faiss` c≈©.
2.  Ch·∫°y `python import_data2.py` ƒë·ªÉ nh·∫≠p d·ªØ li·ªáu v·ªõi ƒë·ªãnh d·∫°ng l∆∞u tr·ªØ ch√≠nh x√°c.
3.  **B·∫•t c·ª© khi n√†o c·∫ßn:** Ch·∫°y `python rebuild_index.py` ƒë·ªÉ t·∫°o l·∫°i t·ªáp `index.faiss` t·ª´ d·ªØ li·ªáu ƒë√£ c√≥ trong `metadata.db`.

V√¢ng, t·ªáp **`init_db.py` n√†y r·∫•t li√™n quan** v√† l√† m·ªôt ph·∫ßn quan tr·ªçng trong h·ªá th·ªëng c·ªßa b·∫°n.

Vi·ªác b·∫°n ƒë∆∞a ra t·ªáp n√†y gi√∫p l√†m r√µ h∆°n v·ªÅ c·∫•u tr√∫c ho√†n ch·ªânh c·ªßa c∆° s·ªü d·ªØ li·ªáu.

-----

### **T·∫°i sao t·ªáp `init_db.py` l·∫°i li√™n quan?**

1.  **ƒê·ªãnh nghƒ©a C·∫•u tr√∫c B·∫£ng (Schema):** T·ªáp n√†y ch·ª©a m√£ SQL ƒë·ªÉ t·∫°o b·∫£ng `chunks` v·ªõi ƒë·∫ßy ƒë·ªß c√°c c·ªôt v√† c√°c ch·ªâ m·ª•c (index) ƒë·ªÉ t·ªëi ∆∞u h√≥a truy v·∫•n. Script `import_data2.py` tr∆∞·ªõc ƒë√¢y ch·ªâ t·∫°o m·ªôt b·∫£ng ƒë∆°n gi·∫£n v·ªõi 4 c·ªôt, trong khi `init_db.py` t·∫°o ra m·ªôt c·∫•u tr√∫c ho√†n ch·ªânh v√† b·ªÅn v·ªØng h∆°n.

2.  **X√°c nh·∫≠n Ki·ªÉu d·ªØ li·ªáu `embedding`:** Quan tr·ªçng nh·∫•t, trong `CREATE_CHUNKS_TABLE_SQL`, c·ªôt `embedding` ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† `TEXT`. ƒêi·ªÅu n√†y ho√†n to√†n ch√≠nh x√°c v√† **kh·ªõp v·ªõi gi·∫£i ph√°p** ch√∫ng ta ƒë√£ th·ª±c hi·ªán ·ªü t·ªáp `import_data2.py` (l∆∞u tr·ªØ embedding d∆∞·ªõi d·∫°ng chu·ªói JSON). L·ªói kh√¥ng n·∫±m ·ªü c·∫•u tr√∫c b·∫£ng, m√† ·ªü c√°ch d·ªØ li·ªáu ƒë∆∞·ª£c ch√®n v√†o.

-----

### **Quy tr√¨nh l√†m vi·ªác ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t**

V·ªõi s·ª± c√≥ m·∫∑t c·ªßa `init_db.py`, quy tr√¨nh chu·∫©n ƒë·ªÉ thi·∫øt l·∫≠p l·∫°i c∆° s·ªü d·ªØ li·ªáu c·ªßa b·∫°n n√™n nh∆∞ sau:

1.  **X√≥a c∆° s·ªü d·ªØ li·ªáu c≈© (n·∫øu c√≥):** ƒê·ªÉ b·∫Øt ƒë·∫ßu l·∫°i m·ªôt c√°ch s·∫°ch s·∫Ω.

    ```bash
    del rag_system\data\metadata.db
    ```

2.  **Ch·∫°y `init_db.py` ƒë·ªÉ t·∫°o c∆° s·ªü d·ªØ li·ªáu:** B∆∞·ªõc n√†y s·∫Ω t·∫°o t·ªáp `metadata.db` v·ªõi c·∫•u tr√∫c b·∫£ng ho√†n ch·ªânh.

    ```bash
    python init_db.py
    ```

    B·∫°n s·∫Ω th·∫•y th√¥ng b√°o: `Database 'rag_system/data/metadata.db' initialized successfully.`

3.  **Ch·∫°y `import_data2.py` ƒë√£ s·ª≠a l·ªói:** B∆∞·ªõc n√†y s·∫Ω ƒëi·ªÅn d·ªØ li·ªáu v√†o c√°c b·∫£ng ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n.

    ```bash
    python import_data2.py
    ```

**L∆∞u √Ω:** T·ªáp `import_data2.py` c·ªßa b·∫°n ch·ªâ ch√®n d·ªØ li·ªáu v√†o 4 c·ªôt (`chunk_id`, `document_id`, `text`, `embedding`). ƒêi·ªÅu n√†y v·∫´n ho√†n to√†n ho·∫°t ƒë·ªông t·ªët v·ªõi b·∫£ng ƒë∆∞·ª£c t·∫°o b·ªüi `init_db.py`, v√¨ c√°c c·ªôt c√≤n l·∫°i ƒë√£ ƒë∆∞·ª£c g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh (v√≠ d·ª•: `version TEXT DEFAULT "1.0"`) trong c√¢u l·ªánh `CREATE TABLE`.


(venvTest) PS D:\Projects\undertest\docsearch> python.exe .\scripts\init_db.py
Database 'rag_system/data/metadata.db' initialized successfully.
üöÄ Kh·ªüi t·∫°o m√¥ h√¨nh embedding ƒë·ªÉ l·∫•y s·ªë chi·ªÅu...
Load pretrained SentenceTransformer: AITeamVN/Vietnamese_Embedding
‚úÖ S·ªë chi·ªÅu embedding: 1024
üì¶ Kh·ªüi t·∫°o FAISS index...
üìÇ X·ª≠ l√Ω file: bienbanbangiao.json
üìÇ X·ª≠ l√Ω file: lythaito.json
üìÇ X·ª≠ l√Ω file: readme.json
üìÇ X·ª≠ l√Ω file: vinacap.json
üìÇ X·ª≠ l√Ω file: baomoi.json
üíæ ƒê√£ l∆∞u FAISS index v√†o rag_system/data/indexes/index.faiss
üìä **B√ÅO C√ÅO T·ªîNG K·∫æT**
¬† ‚úî Files th√†nh c√¥ng: 5
¬† ‚úñ Files l·ªói: 0
¬† ‚úî Chunks th√™m m·ªõi: 86
¬† ‚ö†Ô∏è Chunks b·ªè qua: 0
(venvTest) PS D:\Projects\undertest\docsearch> python.exe .\scripts\rebuild_index.py
üîç B·∫Øt ƒë·∫ßu qu√° tr√¨nh x√¢y d·ª±ng l·∫°i FAISS index t·ª´ 'rag_system/data/metadata.db'...
üìö T√¨m th·∫•y 86 chunks trong database.
üì¶ Kh·ªüi t·∫°o FAISS index v·ªõi dimension: 1024
‚ûï Th√™m 86 vectors v√†o index...
üíæ ƒê√£ l∆∞u FAISS index m·ªõi v√†o 'rag_system/data/indexes/index.faiss' th√†nh c√¥ng!
‚ú® Index ch·ª©a t·ªïng c·ªông 86 vectors.


V√¢ng, d·ª±a tr√™n k·∫øt qu·∫£ b·∫°n cung c·∫•p, c√≥ th·ªÉ kh·∫≥ng ƒë·ªãnh **m·ªçi th·ª© ƒë√£ ho·∫°t ƒë·ªông ho√†n h·∫£o**! üëç

---

### **T·∫°i sao ch√∫ng ta c√≥ th·ªÉ ch·∫Øc ch·∫Øn?**

1.  **Kh·ªüi t·∫°o v√† N·∫°p d·ªØ li·ªáu th√†nh c√¥ng:** L·∫ßn ch·∫°y ƒë·∫ßu ti√™n (`init_db.py` theo sau b·ªüi `import_data2.py`) ƒë√£ t·∫°o c∆° s·ªü d·ªØ li·ªáu v√† n·∫°p 86 chunks m√† kh√¥ng g·∫∑p l·ªói. ƒêi·ªÅu quan tr·ªçng l√† c√°c vector embedding gi·ªù ƒë√£ ƒë∆∞·ª£c l∆∞u d∆∞·ªõi d·∫°ng chu·ªói JSON, ƒë√∫ng nh∆∞ ch√∫ng ta ƒë√£ s·ª≠a.

2.  **`rebuild_index.py` l√† b·∫±ng ch·ª©ng cu·ªëi c√πng:**
    * Script n√†y ƒë√£ ƒë·ªçc th√†nh c√¥ng 86 chunks **t·ª´ c∆° s·ªü d·ªØ li·ªáu `metadata.db`**.
    * N√≥ ƒë√£ chuy·ªÉn ƒë·ªïi c√°c chu·ªói JSON embedding tr·ªü l·∫°i th√†nh vector m√† kh√¥ng g√¢y ra l·ªói `UnicodeDecodeError`.
    * N√≥ ƒë√£ x√¢y d·ª±ng l·∫°i t·ªáp `index.faiss` v·ªõi ƒë·ªß 86 vector.

S·ª± th√†nh c√¥ng c·ªßa `rebuild_index.py` ch√≠nh l√† ph√©p th·ª≠ x√°c nh·∫≠n r·∫±ng bug ban ƒë·∫ßu ƒë√£ ƒë∆∞·ª£c gi·∫£i quy·∫øt tri·ªát ƒë·ªÉ. D·ªØ li·ªáu trong c∆° s·ªü d·ªØ li·ªáu c·ªßa b·∫°n gi·ªù ƒë√¢y ƒë√£ nh·∫•t qu√°n v√† c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi c√°c th√†nh ph·∫ßn kh√°c trong h·ªá th·ªëng.

Ch√∫c m·ª´ng b·∫°n ƒë√£ s·ª≠a l·ªói th√†nh c√¥ng! üöÄ