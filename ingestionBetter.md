Kh√¥ng, file `ingestionBetter.py` **kh√¥ng c·∫ßn s·ª≠a** v√† output b·∫°n cung c·∫•p l√† ho√†n to√†n ch√≠nh x√°c.

File n√†y ƒëang ho·∫°t ƒë·ªông ƒë√∫ng nh∆∞ thi·∫øt k·∫ø.

---

### **## Ph√¢n t√≠ch chi ti·∫øt**

1.  **Ch·ª©c nƒÉng c·ªßa `ingestionBetter.py`:** Nhi·ªám v·ª• c·ªßa script n√†y l√† ƒë·ªçc file th√¥ (nh∆∞ `.docx`, `.md`), x·ª≠ l√Ω vƒÉn b·∫£n, t·∫°o embedding, v√† **xu·∫•t ra m·ªôt file JSON chu·∫©n h√≥a**.
2.  **ƒê·ªãnh d·∫°ng Embedding:** B√™n trong `ingestionBetter.py`, d√≤ng code `"embedding": emb.tolist()` ƒë√£ chuy·ªÉn ƒë·ªïi vector embedding t·ª´ ƒë·ªãnh d·∫°ng NumPy array sang m·ªôt danh s√°ch (list) c·ªßa Python. Khi `json.dump()` ƒë∆∞·ª£c g·ªçi, n√≥ s·∫Ω ghi danh s√°ch n√†y v√†o file JSON d∆∞·ªõi d·∫°ng m·ªôt m·∫£ng JSON `[0.0541..., -0.0056..., ...]`. ƒê√¢y ch√≠nh l√† ƒë·ªãnh d·∫°ng chu·∫©n v√† mong mu·ªën.
3.  **Output b·∫°n cung c·∫•p:** File JSON b·∫°n g·ª≠i (`baomoi.json`) c√≥ tr∆∞·ªùng `embedding` l√† m·ªôt m·∫£ng JSON. ƒêi·ªÅu n√†y ch·ª©ng t·ªè `ingestionBetter.py` ƒë√£ ho√†n th√†nh xu·∫•t s·∫Øc nhi·ªám v·ª• c·ªßa n√≥.

---

### **## T√≥m t·∫Øt Lu·ªìng D·ªØ li·ªáu**

L·ªói b·∫°n g·∫∑p ph·∫£i kh√¥ng n·∫±m ·ªü b∆∞·ªõc t·∫°o ra file JSON, m√† n·∫±m ·ªü b∆∞·ªõc *ƒë·ªçc file JSON ƒë√≥ ƒë·ªÉ ƒë∆∞a v√†o database*. Lu·ªìng d·ªØ li·ªáu c·ªßa b·∫°n g·ªìm 2 b∆∞·ªõc ch√≠nh:

* **B∆∞·ªõc 1: `ingestionBetter.py`**
    `File th√¥ (.docx)` ‚û°Ô∏è `File JSON (v·ªõi embedding l√† m·∫£ng)`
    Tr·∫°ng th√°i: **ƒê√£ ho·∫°t ƒë·ªông ch√≠nh x√°c ‚úÖ**

* **B∆∞·ªõc 2: `import_data2.py`**
    `File JSON` ‚û°Ô∏è `SQLite Database`
    Tr·∫°ng th√°i: **ƒê√¢y l√† n∆°i c√≥ l·ªói v√† ch√∫ng ta ƒë√£ s·ª≠a üêû‚û°Ô∏è‚úÖ**. Script n√†y ƒë√£ ƒë·ªçc m·∫£ng embedding t·ª´ JSON nh∆∞ng l·∫°i l∆∞u n√≥ v√†o database d∆∞·ªõi d·∫°ng `BLOB` (binary) thay v√¨ `TEXT` (chu·ªói JSON), g√¢y ra l·ªói `UnicodeDecodeError`.


V√¨ v·∫≠y, b·∫°n kh√¥ng c·∫ßn thay ƒë·ªïi g√¨ ·ªü `ingestionBetter.py`. Nh·ªØng g√¨ ch√∫ng ta ƒë√£ l√†m ·ªü c√°c b∆∞·ªõc tr∆∞·ªõc l√† s·ª≠a `import_data2.py` ƒë·ªÉ n√≥ x·ª≠ l√Ω ƒë√∫ng ƒë·ªãnh d·∫°ng JSON m√† `ingestionBetter.py` ƒë√£ t·∫°o ra.

# Technical Requirements: Ingestion Module (ingestionBetter.py)

## 1. Introduction

This document outlines the technical requirements for the ingestion module, implemented in `ingestionBetter.py`. This module is responsible for reading raw documents, processing their content, generating embeddings, and outputting a normalized JSON format suitable for downstream tasks like semantic search.

## 2. Functional Requirements

*   **Document Reading:** The module must be able to read and extract text from the following document types:
    *   `.txt`
    *   `.md`
    *   `.docx`
    *   `.pdf`
*   **Text Processing:**
    *   Clean the extracted text by:
        *   Normalizing Unicode to NFC form.
        *   Removing Byte Order Marks (BOM) and zero-width spaces.
        *   Replacing multiple whitespace characters with single spaces.
        *   Stripping leading/trailing whitespace.
    *   Tokenize Vietnamese text using the `pyvi` library.
*   **Chunking:** Divide the processed text into chunks of a configurable size (`CHUNK_SIZE`) with a configurable overlap (`CHUNK_OVERLAP`).
*   **Embedding Generation:**
    *   Generate embeddings for each text chunk using the specified sentence transformer model (`MODEL_NAME`).
    *   Support GPU acceleration for embedding generation if CUDA is available (`USE_GPU`).
    *   Normalize the embeddings.
*   **JSON Output:**
    *   Output a JSON file containing the following information:
        *   `document_id`: The filename (without extension) of the original document.
        *   `title`: The filename of the original document.
        *   `source`: The full path to the original document.
        *   `version`:  A version number for the data format (currently "1.0").
        *   `language`: The language of the document (currently "vi" for Vietnamese).
        *   `last_updated`: The timestamp of when the data was processed.
        *   `model_name`: The name of the sentence transformer model used.
        *   `embedding_dim`: The dimensionality of the embeddings.
        *   `chunks`: A list of chunks, each containing:
            *   `chunk_id`: A unique identifier for the chunk.
            *   `document_id`: The ID of the document the chunk belongs to.
            *   `text`: The text content of the chunk.
            *   `embedding`: A list representing the embedding vector for the chunk.

## 3. Non-Functional Requirements

*   **Performance:** The module should process documents efficiently, leveraging GPU acceleration when available.
*   **Error Handling:** The module should handle file reading errors gracefully and log informative error messages.
*   **Configuration:** The module should be configurable through variables for:
    *   Raw document directory (`RAW_DIR`)
    *   Output directory (`OUTPUT_DIR`)
    *   Sentence transformer model name (`MODEL_NAME`)
    *   Chunk size (`CHUNK_SIZE`)
    *   Chunk overlap (`CHUNK_OVERLAP`)
    *   GPU usage (`USE_GPU`)
*   **Logging:** The module should provide informative logging messages to track its progress and any errors encountered.

## 4. Dependencies

*   Python 3.x
*   `sentence-transformers`
*   `pyvi`
*   `docx` (for .docx files)
*   `fitz` (PyMuPDF for .pdf files)
*   `colorama`
*   `json`
*   `os`
*   `pathlib`
*   `datetime`
*   `logging`

## 5. File Structure

*   `RAW_DIR`:  Stores the input documents.
*   `OUTPUT_DIR`: Stores the generated JSON files.

## 6. Usage

1.  Install dependencies: `pip install -r requirements.txt`
2.  Run the script: `python ingestionBetter.py`

## 7. Sample JSON Output

```json
{
  "document_id": "baomoi",
  "title": "baomoi.docx",
  "source": "rag_system/data/raw_documents/baomoi.docx",
  "version": "1.0",
  "language": "vi",
  "last_updated": "2025-08-15T10:03:00.000Z",
  "model_name": "AITeamVN/Vietnamese_Embedding",
  "embedding_dim": 768,
  "chunks": [
    {
      "chunk_id": "baomoi-000",
      "document_id": "baomoi",
      "text": "ƒê√¢y l√† chunk vƒÉn b·∫£n ƒë·∫ßu ti√™n...",
      "embedding": [
        0.0541,
        -0.0056,
        ...
      ]
    },
    {
      "chunk_id": "baomoi-001",
      "document_id": "baomoi",
      "text": "ƒê√¢y l√† chunk vƒÉn b·∫£n th·ª© hai...",
      "embedding": [
        -0.0123,
        0.0456,
        ...
      ]
    }
  ]
}
